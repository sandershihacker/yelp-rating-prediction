{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Restaurant Rating Prediction\n",
    "\n",
    "## Project Description\n",
    "The project goal is to predict restaurant overall ratings on yelp\n",
    "in New York City, using multiple features of restaurants that we will\n",
    "extract using the yelp API. Our motivation is to help determine how\n",
    "successful a new restaurant business might be, given certain known\n",
    "characteristics of it.\n",
    "\n",
    "We will use the Yelp API, with the help of pandas, to acquire raw data\n",
    "from restaurants. Then we will extract reasonable features such as\n",
    "location, open hours, whether it takes reservations, whether it has\n",
    "delivery service, whether there is parking space, and whether it\n",
    "provides free wifi etc., from the parsed data, and combine with the\n",
    "overall ratings, which is a numerical value ranging from 0 to 5, as\n",
    "labels.\n",
    "\n",
    "We will model the rating distribution over the different features that\n",
    "we extract and create, and analyse how much each feature shifts our\n",
    "distribution. Using our results from this we will select good features\n",
    "to train on machine learning models.\n",
    "\n",
    "Using the labeled features that we construct, we will train different\n",
    "machine learning models like linear regression, nonlinear regression,\n",
    "logistic regression as well as neural networks, then make some\n",
    "predictions, and compare the accuracy obtained from them.\n",
    "\n",
    "## Team Members\n",
    "Jun Hee Kim, Nikhil Rangarajan, Sander Shi\n",
    "\n",
    "## Procedure\n",
    "* [Data Gathering from API](#step-1)\n",
    "* [Feature Extraction with Parsing](#step-2)\n",
    "* [Feature Analysis and Variable Selection](#step-3)\n",
    "* [Setup of Models](#step-4)\n",
    "* [Cross Validation](#step-5)\n",
    "* [Final Analysis](#step-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Imports and Definitions of Constants\n",
    "\n",
    "We will be using `pandas` to parse the data and `tensorflow` to construct the machine learning models. We will also be using the Yelp API to gather the data. In order to\n",
    "use the Yelp API, we need to create an API key to use in our API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "API_URL = \"https://api.yelp.com/v3/businesses\"\n",
    "SEARCH_URL = API_URL + \"/search\"\n",
    "RESTAURANT_IDS = \"rid.pkl\"\n",
    "\n",
    "with open(\"./API_KEY\", 'r') as f:\n",
    "    api_key = f.readline().strip()\n",
    "API_HEADERS = {\n",
    "    'Authorization': ' '.join(['Bearer', api_key])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-1\"></a>\n",
    "\n",
    "## Part 1: Data Gathering from API\n",
    "\n",
    "In this step we will use the Yelp API to gather restaurant pages, then extract\n",
    "information using business search API requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Yelp API will only allow us to load business information from up to 1000\n",
    "distinct businesses on each search, we will first get the categories of the\n",
    "restaurants, then perform a search query for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restaurant_categories(url):\n",
    "    \"\"\"\n",
    "    This function gets all restaurant categories.\n",
    "    \n",
    "    @input url: URL to json file containing restaurant categories.\n",
    "    @type url: String.\n",
    "    \n",
    "    @return: List of restaurant categories.\n",
    "    @rtype: List of String\n",
    "    \"\"\"\n",
    "    cats = json.load(open(url))\n",
    "    return [cat['alias'] for cat in cats if 'restaurants' in cat['parents']]\n",
    "\n",
    "categories = get_restaurant_categories('categories.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the restaurant categories, we can write a function to find all\n",
    "restaurant IDs, and create a Pandas DataFrame. To aid debugging, we will dump\n",
    "the dataframe into a pickle file so that we will not have to regenerate each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_req_json(url, params=None):\n",
    "    response = requests.get(url=url, headers=API_HEADERS, params=params)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def find_restaurants(url, categories):\n",
    "    \"\"\"\n",
    "    This function loads all restaurant data from restaurants in Pittsburgh.\n",
    "    \n",
    "    @input url: The API url.\n",
    "    @type url: String.\n",
    "    @input categories: The restaurant categories.\n",
    "    @type categories: List of String.\n",
    "    \n",
    "    @return: A Pandas DataFrame containing the restaurant URLs.\n",
    "    @rtype: pandas.DataFrame.\n",
    "    \"\"\"\n",
    "    restaurants = []\n",
    "    for cat in categories:\n",
    "        params = {\n",
    "            'term': 'restaurants',\n",
    "            'location': 'NYC',\n",
    "            'categories': cat\n",
    "        }\n",
    "        content = get_req_json(url, params)\n",
    "        total = min(1000, content['total'])\n",
    "        i = 0\n",
    "        while i < total:\n",
    "            limit = min(50, total - i)\n",
    "            params['limit'] = limit\n",
    "            params['offset'] = i\n",
    "            content = get_req_json(url, params)\n",
    "            restaurants.extend([b['id'] for b in content['businesses']])\n",
    "            i += limit\n",
    "    return pd.DataFrame({'id': list(set(restaurants))})\n",
    "\n",
    "restaurants = find_restaurants(SEARCH_URL, categories)\n",
    "pickle.dump(restaurants, open(RESTAURANT_IDS, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UTNSUIY2QW6RYNa0xLa9gg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h5WkewI6U7NjLB_n6WgdvQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yOPbrFZ2ZUsyQWsedP6gUg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2ngodDyMj6M0szyBbGSJ5g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8ghGdEWOkGWUNeGrtdiaDQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58v6bJy_vQLMWxLSMZHvYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4LoroCI3VtQ17dyQa75-Rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1_JZHKkWQBozYQ00fephWQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YRfDdBgWS4T80oJfKmVThA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IUMTe7g19htqG7MXrsP5NQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Uut2fBy53XdIgx1rLWdQeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>zmOISxxP_O96pZFkC1kuVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nrPfcOC4mGnEL-9CpoGhMg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cp72DAW5Ndt9T625RNsMsg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>oJZq1JmKalXXjqWRKfIjwA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>j5Me3YSKfaYHoLu_74SMjA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MFYh1htbCY8naZfkdF9rEQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rzy-FeiJGYS0pA6HnjYv3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>siMk2bf7QomxjAY0hD3BFQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0TNfNUGnW8jzBbCENxIJhQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>h0qowxmqgwjKsh-fdkxvHw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>x1cyB0m8EIR9OE054dgtPg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>avBY-xnfy4_pXCNc6EFHpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tFfPM28udwUVzJCnzDU_8g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HVKKciOXD4k9VWXqgM9hqw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>N4V9aeManzL4PhjZX7iKKw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1qFi36tQEr6hpvubjCkLDw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pJ5lwtfm8am0p--4xfDvlQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>z3jtJycAqyxvjeULfLGL8A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>VUcrIqM40HWWb2sxAS9mLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>9tjTiVC2HepvHwEPdc677g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16052</th>\n",
       "      <td>uaFHoq-a5XqxF-bsOK9_Qg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16053</th>\n",
       "      <td>uYMimeHJ5XBGkyTrg8GXhw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16054</th>\n",
       "      <td>dqXZa8fcvHsFvtZVtwRNkA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16055</th>\n",
       "      <td>SqUXuq03n51BCJ766Hr3cA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16056</th>\n",
       "      <td>6MgQz0SEOYUih2N5JOa6rg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16057</th>\n",
       "      <td>in7QM4vNUTHVZIi7Outetg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16058</th>\n",
       "      <td>VvIzNbkl4PDPzutZdxWCJw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16059</th>\n",
       "      <td>Zs4IZRR5eNH_DaKDNa37wg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16060</th>\n",
       "      <td>hgTjvId9I7Z8CmM3pGeB2g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16061</th>\n",
       "      <td>Hskf7Hd7EkgBMDxrvG54fg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16062</th>\n",
       "      <td>K7kvjBYe_bZxBp6cpNO-SQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16063</th>\n",
       "      <td>dLYFNYqSw2L148bTJbwmiw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16064</th>\n",
       "      <td>71_8yA0L_FCwfgC-8KQjXA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16065</th>\n",
       "      <td>ikx8R8u2lmxhpLGByCjgPw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16066</th>\n",
       "      <td>KZHeT-AgZnF6iYCVsucClg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16067</th>\n",
       "      <td>boU8YGyv3xft7VwH37Bh2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16068</th>\n",
       "      <td>QGvOaL14mWte55v71i3PIQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16069</th>\n",
       "      <td>8EiKqEFhAAOxhUCMV9qFfg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16070</th>\n",
       "      <td>QIimmBBtZc1bJ0jthIx6Bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16071</th>\n",
       "      <td>OBx6NwNScuENCclIK8H5Ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16072</th>\n",
       "      <td>FpPq4U85UJGa-WDioGyJ-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16073</th>\n",
       "      <td>Ps0cOZevNA1M9M5_z7W3oQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16074</th>\n",
       "      <td>ElFVjpPTej8ji4eIFbld6g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16075</th>\n",
       "      <td>L1-qxIGlGqad0mLWhW1PUg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16076</th>\n",
       "      <td>kOFUrzoQPW6F0Bqllz5lpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16077</th>\n",
       "      <td>bHQbqSRY8FDp7fQXPPzx1g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16078</th>\n",
       "      <td>s2xONRCHI08MISGZXzqBhA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>cevkTGf4vX_IBU_bGlz9ww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16080</th>\n",
       "      <td>MV39gToOLxzRdfuzzt-kUA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16081 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id\n",
       "0      UTNSUIY2QW6RYNa0xLa9gg\n",
       "1      h5WkewI6U7NjLB_n6WgdvQ\n",
       "2      yOPbrFZ2ZUsyQWsedP6gUg\n",
       "3      2ngodDyMj6M0szyBbGSJ5g\n",
       "4      8ghGdEWOkGWUNeGrtdiaDQ\n",
       "5      58v6bJy_vQLMWxLSMZHvYA\n",
       "6      4LoroCI3VtQ17dyQa75-Rg\n",
       "7      1_JZHKkWQBozYQ00fephWQ\n",
       "8      YRfDdBgWS4T80oJfKmVThA\n",
       "9      IUMTe7g19htqG7MXrsP5NQ\n",
       "10     Uut2fBy53XdIgx1rLWdQeg\n",
       "11     zmOISxxP_O96pZFkC1kuVA\n",
       "12     nrPfcOC4mGnEL-9CpoGhMg\n",
       "13     cp72DAW5Ndt9T625RNsMsg\n",
       "14     oJZq1JmKalXXjqWRKfIjwA\n",
       "15     j5Me3YSKfaYHoLu_74SMjA\n",
       "16     MFYh1htbCY8naZfkdF9rEQ\n",
       "17     Rzy-FeiJGYS0pA6HnjYv3A\n",
       "18     siMk2bf7QomxjAY0hD3BFQ\n",
       "19     0TNfNUGnW8jzBbCENxIJhQ\n",
       "20     h0qowxmqgwjKsh-fdkxvHw\n",
       "21     x1cyB0m8EIR9OE054dgtPg\n",
       "22     avBY-xnfy4_pXCNc6EFHpg\n",
       "23     tFfPM28udwUVzJCnzDU_8g\n",
       "24     HVKKciOXD4k9VWXqgM9hqw\n",
       "25     N4V9aeManzL4PhjZX7iKKw\n",
       "26     1qFi36tQEr6hpvubjCkLDw\n",
       "27     pJ5lwtfm8am0p--4xfDvlQ\n",
       "28     z3jtJycAqyxvjeULfLGL8A\n",
       "29     VUcrIqM40HWWb2sxAS9mLA\n",
       "...                       ...\n",
       "16051  9tjTiVC2HepvHwEPdc677g\n",
       "16052  uaFHoq-a5XqxF-bsOK9_Qg\n",
       "16053  uYMimeHJ5XBGkyTrg8GXhw\n",
       "16054  dqXZa8fcvHsFvtZVtwRNkA\n",
       "16055  SqUXuq03n51BCJ766Hr3cA\n",
       "16056  6MgQz0SEOYUih2N5JOa6rg\n",
       "16057  in7QM4vNUTHVZIi7Outetg\n",
       "16058  VvIzNbkl4PDPzutZdxWCJw\n",
       "16059  Zs4IZRR5eNH_DaKDNa37wg\n",
       "16060  hgTjvId9I7Z8CmM3pGeB2g\n",
       "16061  Hskf7Hd7EkgBMDxrvG54fg\n",
       "16062  K7kvjBYe_bZxBp6cpNO-SQ\n",
       "16063  dLYFNYqSw2L148bTJbwmiw\n",
       "16064  71_8yA0L_FCwfgC-8KQjXA\n",
       "16065  ikx8R8u2lmxhpLGByCjgPw\n",
       "16066  KZHeT-AgZnF6iYCVsucClg\n",
       "16067  boU8YGyv3xft7VwH37Bh2A\n",
       "16068  QGvOaL14mWte55v71i3PIQ\n",
       "16069  8EiKqEFhAAOxhUCMV9qFfg\n",
       "16070  QIimmBBtZc1bJ0jthIx6Bg\n",
       "16071  OBx6NwNScuENCclIK8H5Ow\n",
       "16072  FpPq4U85UJGa-WDioGyJ-Q\n",
       "16073  Ps0cOZevNA1M9M5_z7W3oQ\n",
       "16074  ElFVjpPTej8ji4eIFbld6g\n",
       "16075  L1-qxIGlGqad0mLWhW1PUg\n",
       "16076  kOFUrzoQPW6F0Bqllz5lpg\n",
       "16077  bHQbqSRY8FDp7fQXPPzx1g\n",
       "16078  s2xONRCHI08MISGZXzqBhA\n",
       "16079  cevkTGf4vX_IBU_bGlz9ww\n",
       "16080  MV39gToOLxzRdfuzzt-kUA\n",
       "\n",
       "[16081 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_features(rids, api_url):\n",
    "    prices, ratings, pickups, deliveries= [], [], [], []\n",
    "    reservations, is_claimed, zipcodes = [], [], []\n",
    "    for idx, rid in rids.iterrows():\n",
    "        url = \"/\".join([api_url, rid['id']])\n",
    "        content = get_req_json(url)\n",
    "        prices.append(len(content['price']) if 'price' in content else 0)\n",
    "        ratings.append(content['rating'] if 'rating' in content else 0)\n",
    "        pickups.append(\"pickup\" in content['transactions'])\n",
    "        deliveries.append(\"delivery\" in content['transactions'])\n",
    "        reservations.append(\"restaurant_reservation\" in content['transactions'])\n",
    "        is_claimed.append(content['is_claimed'])\n",
    "        zipcodes.append(content['location']['zip_code'])\n",
    "    df_dict = {\n",
    "        'Price': prices,\n",
    "        'Pickup': pickups,\n",
    "        'Delivery': deliveries,\n",
    "        'Reservation': reservations,\n",
    "        'Claimed': is_claimed,\n",
    "        'Zipcode': zipcodes,\n",
    "        'Ratings': ratings\n",
    "    }\n",
    "    return pd.DataFrame(df_dict)\n",
    "\n",
    "# restaurants = find_restaurants(SEARCH_URL, categories)\n",
    "# pickle.dump(restaurants, open(RESTAURANT_IDS, \"wb\"))\n",
    "\n",
    "restaurant_ids = pickle.load(open(RESTAURANT_IDS, 'rb'))\n",
    "restaurant_ids\n",
    "# labeled_features = get_features(restaurant_ids, API_URL)\n",
    "# pickle.dump(labeled_features, open(\"features.pkl\", \"wb\"))\n",
    "# labeled_features = pickle.load(open(\"features.pkl\", \"rb\"))\n",
    "# print(len(labeled_features))\n",
    "# labeled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-2\"></a>\n",
    "\n",
    "## Part 2: Feature Extraction with Parsing\n",
    "\n",
    "In this step we will parse the raw features we got from the API and mold them into\n",
    "discrete and continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(raw_data):\n",
    "    \"\"\"\n",
    "    This function extracts the features from raw restaurant data.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "labeled_features = extract_features(all_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"step-3\"></a>\n",
    "\n",
    "## Part 3: Feature Analysis and Variable Selection\n",
    "\n",
    "Now that we have the features, we can separately analyse them and see how each feature\n",
    "contributes to the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Jun Hee's Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-4\"></a>\n",
    "\n",
    "## Part 4: Setup of Models\n",
    "\n",
    "We need to construct a couple machine learning models for us to train to see which\n",
    "one gives us the best testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    \"\"\"\n",
    "    This is a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers=[5, 5]):\n",
    "        self.layers = layers\n",
    "        \n",
    "class LogisticRegression(object):\n",
    "    \"\"\"\n",
    "    This is the logistic regression model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.theta = np.zeros(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-5\"></a>\n",
    "\n",
    "## Part 5: Cross Validation\n",
    "\n",
    "Now we will use cross-validation to determine which model produces the highest\n",
    "validation accuracy for our dataset, then pick this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-6\"></a>\n",
    "\n",
    "## Part 6: Final Analysis\n",
    "\n",
    "Our results were amazing, so yeah."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
