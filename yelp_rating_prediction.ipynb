{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Restaurant Rating Prediction\n",
    "\n",
    "## Project Description\n",
    "The project goal is to predict restaurant overall ratings on yelp\n",
    "in New York City, using multiple features of restaurants that we will\n",
    "extract using the yelp API. Our motivation is to help determine how\n",
    "successful a new restaurant business might be, given certain known\n",
    "characteristics of it.\n",
    "\n",
    "We will use the Yelp API, with the help of pandas, to acquire raw data\n",
    "from restaurants. Then we will extract reasonable features such as\n",
    "location, open hours, whether it takes reservations, whether it has\n",
    "delivery service, whether there is parking space, and whether it\n",
    "provides free wifi etc., from the parsed data, and combine with the\n",
    "overall ratings, which is a numerical value ranging from 0 to 5, as\n",
    "labels.\n",
    "\n",
    "We will model the rating distribution over the different features that\n",
    "we extract and create, and analyse how much each feature shifts our\n",
    "distribution. Using our results from this we will select good features\n",
    "to train on machine learning models.\n",
    "\n",
    "Using the labeled features that we construct, we will train different\n",
    "machine learning models like linear regression, nonlinear regression,\n",
    "logistic regression as well as neural networks, then make some\n",
    "predictions, and compare the accuracy obtained from them.\n",
    "\n",
    "## Team Members\n",
    "Jun Hee Kim, Nikhil Rangarajan, Sander Shi\n",
    "\n",
    "## Procedure\n",
    "* [Data Gathering from API](#step-1)\n",
    "* [Feature Extraction with Parsing](#step-2)\n",
    "* [Feature Analysis and Variable Selection](#step-3)\n",
    "* [Setup of Models](#step-4)\n",
    "* [Cross Validation](#step-5)\n",
    "* [Final Analysis](#step-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Imports and Definitions of Constants\n",
    "\n",
    "We will be using `pandas` to parse the data and `tensorflow` to construct the machine learning models. We will also be using the Yelp API to gather the data. In order to\n",
    "use the Yelp API, we need to create an API key to use in our API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_URL = \"https://api.yelp.com/v3/businesses\"\n",
    "SEARCH_URL = API_URL + \"/search\"\n",
    "API_KEY = \"./API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-1\"></a>\n",
    "\n",
    "## Part 1: Data Gathering from API\n",
    "\n",
    "In this step we will use the Yelp API to gather restaurant pages, then extract\n",
    "information using business search API requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Yelp API will only allow us to load business information from up to 1000\n",
    "distinct businesses on each search, we will first get the categories of the\n",
    "restaurants, then perform a search query for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['afghani', 'african', 'andalusian', 'arabian', 'argentine', 'armenian', 'asianfusion', 'asturian', 'australian', 'austrian', 'baguettes', 'bangladeshi', 'basque', 'bavarian', 'bbq', 'beergarden', 'beerhall', 'beisl', 'belgian', 'bistros', 'blacksea', 'brasseries', 'brazilian', 'breakfast_brunch', 'british', 'buffets', 'bulgarian', 'burgers', 'burmese', 'cafes', 'cafeteria', 'cajun', 'cambodian', 'canteen', 'caribbean', 'catalan', 'cheesesteaks', 'chicken_wings', 'chickenshop', 'chilean', 'chinese', 'comfortfood', 'corsican', 'creperies', 'cuban', 'currysausage', 'cypriot', 'czech', 'czechslovakian', 'danish', 'delis', 'diners', 'dinnertheater', 'dumplings', 'eastern_european', 'eltern_cafes', 'ethiopian', 'filipino', 'fischbroetchen', 'fishnchips', 'flatbread', 'fondue', 'food_court', 'foodstands', 'freiduria', 'french', 'galician', 'gamemeat', 'gastropubs', 'georgian', 'german', 'giblets', 'gluten_free', 'greek', 'guamanian', 'halal', 'hawaiian', 'heuriger', 'himalayan', 'hkcafe', 'honduran', 'hotdog', 'hotdogs', 'hotpot', 'hungarian', 'iberian', 'indonesian', 'indpak', 'international', 'irish', 'island_pub', 'israeli', 'italian', 'japanese', 'jewish', 'kebab', 'kopitiam', 'korean', 'kosher', 'kurdish', 'laos', 'laotian', 'latin', 'lyonnais', 'malaysian', 'meatballs', 'mediterranean', 'mexican', 'mideastern', 'milkbars', 'modern_australian', 'modern_european', 'mongolian', 'moroccan', 'newamerican', 'newcanadian', 'newmexican', 'newzealand', 'nicaraguan', 'nightfood', 'nikkei', 'noodles', 'norcinerie', 'norwegian', 'opensandwiches', 'oriental', 'pakistani', 'panasian', 'parma', 'persian', 'peruvian', 'pfcomercial', 'pita', 'pizza', 'polish', 'polynesian', 'popuprestaurants', 'portuguese', 'potatoes', 'poutineries', 'pubfood', 'raw_food', 'riceshop', 'romanian', 'rotisserie_chicken', 'russian', 'salad', 'sandwiches', 'scandinavian', 'schnitzel', 'scottish', 'seafood', 'serbocroatian', 'signature_cuisine', 'singaporean', 'slovakian', 'soulfood', 'soup', 'southern', 'spanish', 'srilankan', 'steak', 'sud_ouest', 'supperclubs', 'sushi', 'swabian', 'swedish', 'swissfood', 'syrian', 'tabernas', 'taiwanese', 'tapas', 'tapasmallplates', 'tavolacalda', 'tex-mex', 'thai', 'tradamerican', 'traditional_swedish', 'trattorie', 'turkish', 'ukrainian', 'uzbek', 'vegan', 'vegetarian', 'venison', 'vietnamese', 'waffles', 'wok', 'wraps', 'yugoslav']\n"
     ]
    }
   ],
   "source": [
    "def get_restaurant_categories(url):\n",
    "    \"\"\"\n",
    "    This function gets all restaurant categories.\n",
    "    \n",
    "    @input url: URL to json file containing restaurant categories.\n",
    "    @type url: String.\n",
    "    \n",
    "    @return: List of restaurant categories.\n",
    "    @rtype: List of String\n",
    "    \"\"\"\n",
    "    cats = json.load(open(url))\n",
    "    return [cat['alias'] for cat in cats if 'restaurants' in cat['parents']]\n",
    "\n",
    "categories = get_restaurant_categories('categories.json')\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the restaurant categories, we can write a function to find all\n",
    "restaurant IDs, and create a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breakfast_brunch 1500\n",
      "chinese 1700\n",
      "delis 1400\n",
      "italian 1500\n",
      "mexican 1200\n",
      "newamerican 1200\n",
      "pizza 1800\n",
      "sandwiches 1800\n",
      "tradamerican 1100\n"
     ]
    }
   ],
   "source": [
    "def find_restaurants(url, api_key_url, categories):\n",
    "    \"\"\"\n",
    "    This function loads all restaurant data from restaurants in Pittsburgh.\n",
    "    \n",
    "    @input url: The API url.\n",
    "    @type url: String.\n",
    "    \n",
    "    @input api_key_url: The API key url.\n",
    "    @type api_key_url: String.\n",
    "    \n",
    "    @input categories: The restaurant categories.\n",
    "    @type categories: List of String.\n",
    "    \n",
    "    @return: A Pandas DataFrame containing the restaurant URLs.\n",
    "    @rtype: pandas.DataFrame.\n",
    "    \"\"\"\n",
    "    # Retrieve API key\n",
    "    with open(api_key_url, 'r') as f:\n",
    "        api_key = f.readline().strip()\n",
    "        \n",
    "    # Set request header and params for search query\n",
    "    headers = {\n",
    "        'Authorization': ' '.join(['Bearer', api_key])\n",
    "    }\n",
    "    params = {\n",
    "        'term': 'restaurants',\n",
    "        'location': 'NYC',\n",
    "        'limit': 15,\n",
    "        'offset': 0\n",
    "    }\n",
    "    \n",
    "    # Loop through categories\n",
    "    for cat in categories:\n",
    "        params['categories'] = cat\n",
    "        response = requests.get(url=url, headers=headers, params=params)\n",
    "        content = response.json()\n",
    "        total = 1000 if content['total'] > 1000 else content['total']\n",
    "        for \n",
    "    return None\n",
    "\n",
    "restaurants = find_restaurants(SEARCH_URL, API_KEY, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-2\"></a>\n",
    "\n",
    "## Part 2: Feature Extraction with Parsing\n",
    "\n",
    "In this step we will parse the raw features we got from the API and mold them into\n",
    "discrete and continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(raw_data):\n",
    "    \"\"\"\n",
    "    This function extracts the features from raw restaurant data.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "labeled_features = extract_features(all_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"step-3\"></a>\n",
    "\n",
    "## Part 3: Feature Analysis and Variable Selection\n",
    "\n",
    "Now that we have the features, we can separately analyse them and see how each feature\n",
    "contributes to the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Jun Hee's Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-4\"></a>\n",
    "\n",
    "## Part 4: Setup of Models\n",
    "\n",
    "We need to construct a couple machine learning models for us to train to see which\n",
    "one gives us the best testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    \"\"\"\n",
    "    This is a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers=[5, 5]):\n",
    "        self.layers = layers\n",
    "        \n",
    "class LogisticRegression(object):\n",
    "    \"\"\"\n",
    "    This is the logistic regression model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.theta = np.zeros(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-5\"></a>\n",
    "\n",
    "## Part 5: Cross Validation\n",
    "\n",
    "Now we will use cross-validation to determine which model produces the highest\n",
    "validation accuracy for our dataset, then pick this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-6\"></a>\n",
    "\n",
    "## Part 6: Final Analysis\n",
    "\n",
    "Our results were amazing, so yeah."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
