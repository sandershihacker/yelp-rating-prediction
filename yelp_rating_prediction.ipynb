{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Restaurant Rating Prediction\n",
    "\n",
    "## Project Description\n",
    "The project goal is to predict restaurant overall ratings on yelp\n",
    "in New York City, using multiple features of restaurants that we will\n",
    "extract using the yelp API. Our motivation is to help determine how\n",
    "successful a new restaurant business might be, given certain known\n",
    "characteristics of it.\n",
    "\n",
    "We will use the Yelp API, with the help of pandas, to acquire raw data\n",
    "from restaurants. Then we will extract reasonable features such as\n",
    "location, open hours, whether it takes reservations, whether it has\n",
    "delivery service, whether there is parking space, and whether it\n",
    "provides free wifi etc., from the parsed data, and combine with the\n",
    "overall ratings, which is a numerical value ranging from 0 to 5, as\n",
    "labels.\n",
    "\n",
    "We will model the rating distribution over the different features that\n",
    "we extract and create, and analyse how much each feature shifts our\n",
    "distribution. Using our results from this we will select good features\n",
    "to train on machine learning models.\n",
    "\n",
    "Using the labeled features that we construct, we will train different\n",
    "machine learning models like linear regression, nonlinear regression,\n",
    "logistic regression as well as neural networks, then make some\n",
    "predictions, and compare the accuracy obtained from them.\n",
    "\n",
    "## Team Members\n",
    "Jun Hee Kim, Nikhil Rangarajan, Sander Shi\n",
    "\n",
    "## Procedure\n",
    "* [Data Gathering from API](#step-1)\n",
    "* [Feature Extraction with Parsing](#step-2)\n",
    "* [Feature Analysis and Variable Selection](#step-3)\n",
    "* [Setup of Models](#step-4)\n",
    "* [Cross Validation](#step-5)\n",
    "* [Final Analysis](#step-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Imports and Definitions of Constants\n",
    "\n",
    "We will be using `pandas` to parse the data and `tensorflow` to construct the machine learning models. We will also be using the Yelp API to gather the data. In order to\n",
    "use the Yelp API, we need to create an API key to use in our API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "API_URL = \"https://api.yelp.com/v3/businesses\"\n",
    "SEARCH_URL = API_URL + \"/search\"\n",
    "RESTAURANT_IDS = \"rid.pkl\"\n",
    "\n",
    "with open(\"./API_KEY\", 'r') as f:\n",
    "    api_key = f.readline().strip()\n",
    "API_HEADERS = {\n",
    "    'Authorization': ' '.join(['Bearer', api_key])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-1\"></a>\n",
    "\n",
    "## Part 1: Data Gathering from API\n",
    "\n",
    "In this step we will use the Yelp API to gather restaurant pages, then extract\n",
    "information using business search API requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Yelp API will only allow us to load business information from up to 1000\n",
    "distinct businesses on each search, we will first get the categories of the\n",
    "restaurants, then perform a search query for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restaurant_categories(url):\n",
    "    \"\"\"\n",
    "    This function gets all restaurant categories.\n",
    "    \n",
    "    @input url: URL to json file containing restaurant categories.\n",
    "    @type url: String.\n",
    "    \n",
    "    @return: List of restaurant categories.\n",
    "    @rtype: List of String\n",
    "    \"\"\"\n",
    "    cats = json.load(open(url))\n",
    "    return [cat['alias'] for cat in cats if 'restaurants' in cat['parents']]\n",
    "\n",
    "categories = get_restaurant_categories('categories.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the restaurant categories, we can write a function to find all\n",
    "restaurant IDs, and create a Pandas DataFrame. To aid debugging, we will dump\n",
    "the dataframe into a pickle file so that we will not have to regenerate each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_req_json(url, params=None):\n",
    "    response = requests.get(url=url, headers=API_HEADERS, params=params)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def find_restaurants(url, categories):\n",
    "    \"\"\"\n",
    "    This function loads all restaurant data from restaurants in Pittsburgh.\n",
    "    \n",
    "    @input url: The API url.\n",
    "    @type url: String.\n",
    "    @input categories: The restaurant categories.\n",
    "    @type categories: List of String.\n",
    "    \n",
    "    @return: A Pandas DataFrame containing the restaurant URLs.\n",
    "    @rtype: pandas.DataFrame.\n",
    "    \"\"\"\n",
    "    restaurants = []\n",
    "    for cat in categories:\n",
    "        params = {\n",
    "            'term': 'restaurants',\n",
    "            'location': 'NYC',\n",
    "            'categories': cat\n",
    "        }\n",
    "        content = get_req_json(url, params)\n",
    "        total = min(1000, content['total'])\n",
    "        i = 0\n",
    "        while i < total:\n",
    "            limit = min(50, total - i)\n",
    "            params['limit'] = limit\n",
    "            params['offset'] = i\n",
    "            content = get_req_json(url, params)\n",
    "            restaurants.extend([b['id'] for b in content['businesses']])\n",
    "            i += limit\n",
    "    return pd.DataFrame({'id': list(set(restaurants))})\n",
    "\n",
    "restaurants = find_restaurants(SEARCH_URL, categories)\n",
    "pickle.dump(restaurants, open(RESTAURANT_IDS, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-2\"></a>\n",
    "\n",
    "## Part 2: Feature Extraction with Parsing\n",
    "\n",
    "Now that we have all the restaurant ids, in this step we can do a business lookup\n",
    "to get the features from the restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_features(rids, api_url):\n",
    "    prices, ratings, pickups, deliveries= [], [], [], []\n",
    "    reservations, is_claimed, zipcodes = [], [], []\n",
    "    \n",
    "    timer = 0\n",
    "    \n",
    "    for idx, rid in rids.iterrows():\n",
    "        \n",
    "        if timer % 500 == 0:\n",
    "            print(timer)\n",
    "        timer += 1\n",
    "        \n",
    "        url = \"/\".join([api_url, rid['id']])\n",
    "        content = get_req_json(url)\n",
    "        prices.append(len(content['price']) if 'price' in content else 0)\n",
    "        ratings.append(float(getattr(content, 'rating', 0)))\n",
    "        transactions = getattr(content, 'transactions', [])\n",
    "        pickups.append(\"pickup\" in transactions)\n",
    "        deliveries.append(\"delivery\" in 'transactions')\n",
    "        reservations.append(\"restaurant_reservation\" in 'transactions')\n",
    "        is_claimed.append(getattr(content, 'is_claimed', False))\n",
    "        if ('location' not in content) or ('zip_code' not in content['location']):\n",
    "            zipcodes.append('0')\n",
    "        else:\n",
    "            zipcodes.append(content['location']['zip_code'])\n",
    "    df_dict = {\n",
    "        'Price': prices,\n",
    "        'Pickup': pickups,\n",
    "        'Delivery': deliveries,\n",
    "        'Reservation': reservations,\n",
    "        'Claimed': is_claimed,\n",
    "        'Zipcode': zipcodes,\n",
    "        'Ratings': ratings\n",
    "    }\n",
    "    return pd.DataFrame(df_dict)\n",
    "\n",
    "\n",
    "restaurant_ids = pickle.load(open(RESTAURANT_IDS, 'rb'))\n",
    "labeled_features = get_features(restaurant_ids, API_URL)\n",
    "pickle.dump(labeled_features, open(\"features.pkl\", \"wb\"))\n",
    "labeled_features = pickle.load(open(\"features.pkl\", \"rb\"))\n",
    "labeled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"step-3\"></a>\n",
    "\n",
    "## Part 3: Feature Analysis and Variable Selection\n",
    "\n",
    "Now that we have the features, we can separately analyse them and see how each feature\n",
    "contributes to the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Jun Hee's Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-4\"></a>\n",
    "\n",
    "## Part 4: Setup of Models\n",
    "\n",
    "We need to construct a couple machine learning models for us to train to see which\n",
    "one gives us the best testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    \"\"\"\n",
    "    This is a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers=[5, 5]):\n",
    "        self.layers = layers\n",
    "        \n",
    "class LogisticRegression(object):\n",
    "    \"\"\"\n",
    "    This is the logistic regression model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.theta = np.zeros(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-5\"></a>\n",
    "\n",
    "## Part 5: Cross Validation\n",
    "\n",
    "Now we will use cross-validation to determine which model produces the highest\n",
    "validation accuracy for our dataset, then pick this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step-6\"></a>\n",
    "\n",
    "## Part 6: Final Analysis\n",
    "\n",
    "Our results were amazing, so yeah."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
